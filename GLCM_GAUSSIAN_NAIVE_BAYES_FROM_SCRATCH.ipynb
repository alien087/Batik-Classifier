{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import timeit\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "import matplotlib.image as mpimg \n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from PIL import ImageEnhance\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_8bit_to_3bit = [i // 32 for i in range(256)]\n",
    "predicts=[]\n",
    "imgs = {}\n",
    "imgs[\"cap\"] = []\n",
    "imgs[\"tulis\"] = []\n",
    "val_imgs = {}\n",
    "val_imgs[\"cap\"] = []\n",
    "val_imgs[\"tulis\"] = []\n",
    "model = {}\n",
    "model_test = {}\n",
    "neighbor = 3\n",
    "time_modelling_start = timeit.default_timer()\n",
    "model[\"cap\"] = []\n",
    "model[\"tulis\"] = []\n",
    "model_test[\"cap\"] = []\n",
    "model_test[\"tulis\"] = []\n",
    "mean = {}\n",
    "variance = {}\n",
    "mean[\"tulis\"] = []\n",
    "variance[\"tulis\"] = []\n",
    "mean[\"cap\"] = []\n",
    "variance[\"cap\"] = []\n",
    "a1 = 0\n",
    "b1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    global map_8bit_to_3bit\n",
    "    global model\n",
    "    global model_test\n",
    "    global time_modelling_start\n",
    "    global mean\n",
    "    global variance\n",
    "    global a1\n",
    "    global b1\n",
    "    map_8bit_to_3bit = [i // 32 for i in range(256)]\n",
    "    model = {}\n",
    "    model_test = {}\n",
    "    neighbor = 3\n",
    "    time_modelling_start = timeit.default_timer()\n",
    "    model[\"cap\"] = []\n",
    "    model[\"tulis\"] = []\n",
    "    model_test[\"cap\"] = []\n",
    "    model_test[\"tulis\"] = []\n",
    "    mean = {}\n",
    "    variance = {}\n",
    "    mean[\"tulis\"] = []\n",
    "    variance[\"tulis\"] = []\n",
    "    mean[\"cap\"] = []\n",
    "    variance[\"cap\"] = []\n",
    "    a1 =0\n",
    "    b1=0\n",
    "    predicts=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Processing\n",
    "def load_img(img_path):\n",
    "    return Image.open(img_path).convert('L')\n",
    "\n",
    "def print_img(img):\n",
    "    plt.imshow(img)\n",
    "    \n",
    "def get_img_colors(img):\n",
    "    return list(img.getdata())\n",
    "\n",
    "\n",
    "def get_3bit_colors(img):\n",
    "    img_colors = get_img_colors(img)\n",
    "    loop_count = 0\n",
    "    \n",
    "    for img_color in img_colors:\n",
    "        img_colors[loop_count] = map_8bit_to_3bit[img_color]\n",
    "        \n",
    "        loop_count += 1\n",
    "        \n",
    "    return img_colors\n",
    "\n",
    "def get_img_matrix(img_colors):\n",
    "    img_matrix = [] \n",
    "    loop_count = 0\n",
    "    \n",
    "    loop_count = 0\n",
    "    img_square_dimension = int(math.sqrt(len(img_colors)))\n",
    "    \n",
    "    for row in range(img_square_dimension):\n",
    "        temp_row = []\n",
    "        for col in range(img_square_dimension):\n",
    "            temp_row.append(img_colors[loop_count])\n",
    "            \n",
    "            loop_count += 1\n",
    "        img_matrix.append(temp_row)\n",
    "        \n",
    "    return img_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features Extraction\n",
    "def feature_extract(img, sampling_count, class_name):\n",
    "    glcm_component=[]\n",
    "    img_3bit_colors = get_3bit_colors(img)\n",
    "    img_matrix = get_img_matrix(img_3bit_colors)\n",
    "    glcm_matrix=greycomatrix(img_matrix, distances=[1], angles=[0], levels=12, symmetric=False, normed=False)\n",
    "    component = ['contrast', 'homogeneity', 'energy', 'correlation', 'ASM', 'dissimilarity']\n",
    "    for x in component:\n",
    "        glcm_component.append(greycoprops(glcm_matrix, x)[0][0])\n",
    "    glcm_component.append(class_name)\n",
    "    return(glcm_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling\n",
    "def get_class_names(training_folder_path):\n",
    "    return os.listdir(training_folder_path)\n",
    "\n",
    "def make_a_model(class_name, features, learning_rate):\n",
    "    random_splitter = random.uniform(0, 1)\n",
    "    if(random_splitter <= learning_rate):\n",
    "        model[class_name].append(features)\n",
    "    else:\n",
    "        model_test[class_name].append(features)\n",
    "        \n",
    "def to_dataframe(model):\n",
    "    models = pd.DataFrame(model['cap'] + model['tulis'], columns=['contrast', 'homogeneity', 'energy', 'correlation', 'ASM', 'dissimilarity', 'class'])\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "def naive_bayes(image, cap_probability, tulis_probability):\n",
    "    prob = 1\n",
    "    probability = 0.00\n",
    "    for class_name in class_names:\n",
    "        if(class_name==\"tulis\"):\n",
    "            prob=tulis_probability\n",
    "        elif(class_name==\"cap\"):\n",
    "            prob=cap_probability\n",
    "        for i in range(6):\n",
    "            a = (image[i]-mean[class_name][i])**2/(2*variance[class_name][i])\n",
    "            b = 1/(np.sqrt(2*3.14*variance[class_name][i]))\n",
    "            prob=prob * (b* np.exp(0-a))\n",
    "       \n",
    "        if(prob>probability):\n",
    "            probability = prob\n",
    "            probability_class = class_name\n",
    "        prob=1\n",
    "    return probability_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "def testing(cap_probability, tulis_probability):\n",
    "    right = 0\n",
    "    total = 0\n",
    "    for class_name in class_names:\n",
    "        for image in model_test[class_name]:\n",
    "            predict = naive_bayes(image, cap_probability, tulis_probability)\n",
    "            class_image = class_name\n",
    "            if(predict == class_name):\n",
    "                   right+=1\n",
    "            total +=1\n",
    "    accuracy = (right/total) * 100\n",
    "    print(\"Accuracy: \" + str(accuracy) +\"%\\n\")\n",
    "    \n",
    "    \n",
    "def testing_out(sampling_count):\n",
    "    testing_img_paths = glob.glob(test_folder_path + '/' + img_type)\n",
    "    for testing_img_path in testing_img_paths:\n",
    "        imgs = load_img(testing_img_path)\n",
    "        predict_class = knn(imgs, sampling_count, neighbor)\n",
    "        print(\"Categorized as \" + predict_class)\n",
    "        plt.imshow(Image.open(testing_img_path))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testings():\n",
    "    global a1, b1\n",
    "    total={}\n",
    "    total[\"tulis\"] = models.loc[models['class'] == 'tulis']\n",
    "    total[\"cap\"] = models.loc[models['class'] == 'cap']\n",
    "    mean[\"tulis\"] = []\n",
    "    variance[\"tulis\"] = []\n",
    "    mean[\"cap\"] = []\n",
    "    variance[\"cap\"] = []\n",
    "    tulis_probability = (total[\"tulis\"].count()/models.count())[0]\n",
    "    cap_probability = (total[\"cap\"].count()/models.count())[0]\n",
    "    a1=tulis_probability\n",
    "    b1=cap_probability\n",
    "    for a in [\"tulis\", \"cap\"]:\n",
    "        for b in ['contrast', 'homogeneity', 'energy', 'correlation', 'ASM', 'dissimilarity']:\n",
    "            mean[a].append(total[a][b].mean())\n",
    "            variance[a].append(total[a][b].var())\n",
    "    \n",
    "    testing(cap_probability, tulis_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation\n",
    "def validate(sampling_count, a, b, val_image_features, iterate, epoch):\n",
    "    fp_tulis, fn_tulis, tp_tulis, tn_tulis = 0,0,0,0\n",
    "    fp_cap, fn_cap, tp_cap, tn_cap = 0,0,0,0\n",
    "    right = 0\n",
    "    total = 0\n",
    "    for class_name in class_names:\n",
    "        for image in val_image_features[class_name]:\n",
    "            predict = naive_bayes(image,a,b)\n",
    "            predicts.append(predict)\n",
    "            class_image = class_name\n",
    "            if(predict == class_name):\n",
    "                right+=1\n",
    "\n",
    "            total +=1\n",
    "        \n",
    "        accuracy = (right/total) * 100\n",
    "    print(\"Accuracy: \" + str(accuracy) + \" \"+ str(right) + \"%\\n\")\n",
    "    peformance(iterate, epoch)\n",
    "\n",
    "def peformance(iterate, epoch):\n",
    "    \n",
    "    fp_tulis, fn_tulis, tp_tulis, tn_tulis = 0,0,0,0\n",
    "    fp_cap, fn_cap, tp_cap, tn_cap = 0,0,0,0\n",
    "\n",
    "    for class_name in class_names:\n",
    "        for i in range(118): #Class = Cap\n",
    "            if(predicts[i] == \"cap\"):\n",
    "                if(class_name == \"cap\"):\n",
    "                    tp_cap+=1\n",
    "                else:\n",
    "                    tn_tulis+=1\n",
    "            else:\n",
    "                if(class_name == \"cap\"):\n",
    "                    fp_cap+=1\n",
    "                else:\n",
    "                    fn_tulis+=1\n",
    "\n",
    "        for j in range(i+1, 256): #Class = Tulis\n",
    "            if(predicts[j] == \"tulis\"):\n",
    "                if(class_name == \"cap\"):\n",
    "                    tn_cap+=1\n",
    "                else:\n",
    "                    tp_tulis+=1\n",
    "            else:\n",
    "                if(class_name == \"cap\"):\n",
    "                    fn_cap+=1\n",
    "                else:\n",
    "                    fp_tulis+=1\n",
    "\n",
    "    recall_tulis = round(((tp_tulis / (tp_tulis + fn_tulis))*100),4)\n",
    "    precission_tulis = round(((tp_tulis / (tp_tulis + fp_tulis))*100),4)\n",
    "    f1_score_tulis =  2 * (recall_tulis*precission_tulis) / (recall_tulis + precission_tulis)\n",
    "\n",
    "    recall_cap = round(((tp_cap / (tp_cap + fn_cap))*100),4)\n",
    "    precission_cap = round(((tp_cap / (tp_cap + fp_cap))*100),4)\n",
    "    f1_score_cap=  2 * (recall_cap*precission_cap) / (recall_cap + precission_cap)\n",
    "    \n",
    "    recall = (recall_cap+recall_tulis)/2\n",
    "    precission = (precission_cap + precission_tulis)/2\n",
    "    f1_score = (f1_score_cap + f1_score_tulis)/2\n",
    "    \n",
    " \n",
    "    csvRow = [\"class\", \"Recall\", \"Precission\", \"F1-Score\"]\n",
    "    csvfile = \"Peformance/10_iterasisss/\"+str(epoch+1)+\"_\"+str(iterate+1)+\".csv\"\n",
    "    with open(csvfile, \"w\") as fp:\n",
    "        wr = csv.writer(fp, dialect='excel')\n",
    "        wr.writerow(csvRow)\n",
    "        wr.writerow(['cap', recall_cap, precission_cap, f1_score_cap])\n",
    "        wr.writerow(['tulis', recall_tulis, precission_tulis, f1_score_tulis])\n",
    "        wr.writerow(['avg/total', recall, precission, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch ke-1 \n",
      "Accuracy: 81.74603174603175%\n",
      "\n",
      "Iterasi ke-1 \n",
      "Accuracy: 76.953125 197%\n",
      "\n",
      "Time elapsed to make this model is 101.64262329999997 seconds.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.8\n",
    "sampling_count = 128\n",
    "img_type = '*.jpg'\n",
    "root_path = './'\n",
    "training_folder_path = root_path + 'glcm/training/'\n",
    "validation_folder_path = root_path + 'glcm/validation/'\n",
    "test_folder_path = root_path + 'glcm/test/'\n",
    "\n",
    "class_names = get_class_names(training_folder_path)\n",
    "\n",
    "for class_name in class_names:\n",
    "    training_img_paths = glob.glob(training_folder_path + class_name + '/' + img_type)\n",
    "    validate_img_paths = glob.glob(validation_folder_path + class_name + '/' +img_type)\n",
    "    \n",
    "    for training_img_path in training_img_paths:\n",
    "        training_img = load_img(training_img_path)\n",
    "        enhancer = ImageEnhance.Sharpness(training_img)\n",
    "        training_img = enhancer.enhance(1.5)\n",
    "        training_img_class_name = class_name\n",
    "        feature = feature_extract(training_img, sampling_count, training_img_class_name)\n",
    "        imgs[class_name].append(feature)\n",
    "        \n",
    "    for validate_img_path in validate_img_paths:\n",
    "            img = load_img(validate_img_path)\n",
    "            enhancer = ImageEnhance.Sharpness(img)\n",
    "            img = enhancer.enhance(1.5)\n",
    "            validate_img_class_name = class_name\n",
    "            val_image_feature = feature_extract(img, sampling_count, validate_img_class_name)\n",
    "            val_imgs[class_name].append(val_image_feature)\n",
    "    \n",
    "\n",
    "\n",
    "for j in range(1):\n",
    "    for i in range(1):\n",
    "        for class_name in class_names:\n",
    "            for feature in imgs[class_name]:\n",
    "                make_a_model(class_name, feature, learning_rate)\n",
    "\n",
    "        models = to_dataframe(model)\n",
    "        models_test = to_dataframe(model_test)\n",
    "        print(\"Epoch ke-\"+ str(i+1) + \" \")\n",
    "        testings()\n",
    "        \n",
    "    print(\"Iterasi ke-\"+ str(j+1) + \" \")\n",
    "    validate(128,a1,b1, val_imgs, j, i)\n",
    "    time_modelling_stop = timeit.default_timer()\n",
    "    print('Time elapsed to make this model is ' + str(time_modelling_stop - time_modelling_start) + ' seconds.')\n",
    "    reset()\n",
    "imgs[\"cap\"] = []\n",
    "imgs[\"tulis\"] = []\n",
    "val_imgs[\"cap\"] = []\n",
    "val_imgs[\"tulis\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
